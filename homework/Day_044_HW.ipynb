{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 作業\n",
    "\n",
    "1. 試著調整 RandomForestClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "2. 改用其他資料集 (boston, wine)，並與回歸模型與決策樹的結果進行比較"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets, metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy:  0.9736842105263158\n",
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Feature importance:  [0.05902046 0.01731169 0.45826962 0.46539824]\n"
     ]
    }
   ],
   "source": [
    "# 讀取鳶尾花資料集\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "acc = metrics.accuracy_score(y_test, y_pred)\n",
    "print(\"Acuuracy: \", acc)\n",
    "\n",
    "print(iris.feature_names)\n",
    "\n",
    "print(\"Feature importance: \", clf.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_estimator_3: 0.9210526315789473\n",
      "Feature importance of 3 estimator:[0.05364942 0.02493972 0.57058711 0.35082375]\n",
      "\n",
      "Acuuracy_estimator_10: 0.9736842105263158\n",
      "Feature importance of 10 estimator:[0.09172838 0.04114777 0.39751336 0.46961048]\n",
      "\n",
      "Acuuracy_estimator_100: 0.9736842105263158\n",
      "Feature importance of 100 estimator:[0.11269652 0.02914429 0.36418094 0.49397825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 試著調整 RandomForestClassifier(...) 中的參數，並觀察是否會改變結果？\n",
    "# 可調整參數：n_estimators, criterion, max_features, max_depth, min_samples_split, min_samples_leaf.\n",
    "\n",
    "clf = []\n",
    "y_pred = []\n",
    "acc = []\n",
    "\n",
    "# case-1:n_estimators\n",
    "n_estimators_list = [3, 10, 100]\n",
    "\n",
    "for i in n_estimators_list:\n",
    "    clf.append(RandomForestClassifier(n_estimators = i))\n",
    "\n",
    "# 訓練模型\n",
    "for i in np.arange(0, 3):\n",
    "    clf[i].fit(x_train, y_train)\n",
    "    y_pred.append(clf[i].predict(x_test))\n",
    "    acc.append(metrics.accuracy_score(y_test, y_pred[i]))\n",
    "    print(f\"Acuuracy_estimator_{n_estimators_list[i]}: {acc[i]}\")\n",
    "    print(f\"Feature importance of {n_estimators_list[i]} estimator:{clf[i].feature_importances_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_gini: 0.9473684210526315\n",
      "Feature importance of gini:[0.07866104 0.01152948 0.45117321 0.45863627]\n",
      "\n",
      "Acuuracy_entropy: 0.9736842105263158\n",
      "Feature importance of entropy:[0.23362791 0.05231795 0.2601374  0.45391674]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = []\n",
    "y_pred = []\n",
    "acc = []\n",
    "\n",
    "# case-2:criterion\n",
    "criterion_list = [\"gini\", \"entropy\"]\n",
    "\n",
    "for i in criterion_list:\n",
    "    clf.append(RandomForestClassifier(criterion = i))\n",
    "    \n",
    "for i in np.arange(0, 2):\n",
    "    clf[i].fit(x_train, y_train)\n",
    "    y_pred.append(clf[i].predict(x_test))\n",
    "    acc.append(metrics.accuracy_score(y_test, y_pred[i]))\n",
    "    print(f\"Acuuracy_{criterion_list[i]}: {acc[i]}\")\n",
    "    print(f\"Feature importance of {criterion_list[i]}:{clf[i].feature_importances_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_2: 0.9736842105263158\n",
      "Feature importance of 2 features:[0.09232627 0.01780708 0.40371113 0.48615552]\n",
      "\n",
      "Acuuracy_4: 0.9736842105263158\n",
      "Feature importance of 4 features:[0.01813436 0.01707169 0.484292   0.48050195]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = []\n",
    "y_pred = []\n",
    "acc = []\n",
    "\n",
    "# case-3:max_features\n",
    "max_features_list = [2, 4]\n",
    "\n",
    "for i in max_features_list:\n",
    "    clf.append(RandomForestClassifier(max_features = i))\n",
    "    \n",
    "for i in np.arange(0, 2):\n",
    "    clf[i].fit(x_train, y_train)\n",
    "    y_pred.append(clf[i].predict(x_test))\n",
    "    acc.append(metrics.accuracy_score(y_test, y_pred[i]))\n",
    "    print(f\"Acuuracy_{max_features_list[i]}: {acc[i]}\")\n",
    "    print(f\"Feature importance of {max_features_list[i]} features:{clf[i].feature_importances_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_2: 0.9736842105263158\n",
      "Feature importance of 2 depth:[0.07065076 0.00145008 0.44247422 0.48542493]\n",
      "\n",
      "Acuuracy_10: 0.9736842105263158\n",
      "Feature importance of 10 depth:[0.09789122 0.02877368 0.35356636 0.51976874]\n",
      "\n",
      "Acuuracy_100: 0.9473684210526315\n",
      "Feature importance of 100 depth:[0.14254955 0.07890961 0.41157722 0.36696362]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = []\n",
    "y_pred = []\n",
    "acc = []\n",
    "\n",
    "# case-4:max_depth\n",
    "max_depth_list = [2, 10, 100]\n",
    "\n",
    "for i in max_depth_list:\n",
    "    clf.append(RandomForestClassifier(max_depth = i))\n",
    "    \n",
    "for i in np.arange(0, 3):\n",
    "    clf[i].fit(x_train, y_train)\n",
    "    y_pred.append(clf[i].predict(x_test))\n",
    "    acc.append(metrics.accuracy_score(y_test, y_pred[i]))\n",
    "    print(f\"Acuuracy_{max_depth_list[i]}: {acc[i]}\")\n",
    "    print(f\"Feature importance of {max_depth_list[i]} depth:{clf[i].feature_importances_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_2: 0.9736842105263158\n",
      "Feature importance of 2 min_samples_split:[0.29101009 0.0644555  0.37903054 0.26550387]\n",
      "\n",
      "Acuuracy_10: 0.9736842105263158\n",
      "Feature importance of 10 min_samples_split:[0.08101213 0.03382717 0.53879044 0.34637026]\n",
      "\n",
      "Acuuracy_100: 0.21052631578947367\n",
      "Feature importance of 100 min_samples_split:[0. 0. 0. 0.]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = []\n",
    "y_pred = []\n",
    "acc = []\n",
    "\n",
    "# case-5:min_samples_split\n",
    "min_samples_split_list = [2, 10, 100]\n",
    "\n",
    "for i in min_samples_split_list:\n",
    "    clf.append(RandomForestClassifier(min_samples_split = i))\n",
    "    \n",
    "for i in np.arange(0, 3):\n",
    "    clf[i].fit(x_train, y_train)\n",
    "    y_pred.append(clf[i].predict(x_test))\n",
    "    acc.append(metrics.accuracy_score(y_test, y_pred[i]))\n",
    "    print(f\"Acuuracy_{min_samples_split_list[i]}: {acc[i]}\")\n",
    "    print(f\"Feature importance of {max_depth_list[i]} min_samples_split:{clf[i].feature_importances_}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acuuracy_2: 0.9736842105263158\n",
      "Feature importance of 2 min_samples_leaf:[0.09056879 0.02316977 0.53880815 0.34745328]\n",
      "\n",
      "Acuuracy_10: 0.9736842105263158\n",
      "Feature importance of 10 min_samples_leaf:[0.25784083 0.00316397 0.39486116 0.34413404]\n",
      "\n",
      "Acuuracy_30: 0.868421052631579\n",
      "Feature importance of 30 min_samples_leaf:[0.1 0.  0.7 0.2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = []\n",
    "y_pred = []\n",
    "acc = []\n",
    "\n",
    "# case-6:min_samples_leaf\n",
    "min_samples_leaf_list = [2, 10, 30]\n",
    "\n",
    "for i in min_samples_leaf_list:\n",
    "    clf.append(RandomForestClassifier(min_samples_leaf = i))\n",
    "    \n",
    "for i in np.arange(0, 3):\n",
    "    clf[i].fit(x_train, y_train)\n",
    "    y_pred.append(clf[i].predict(x_test))\n",
    "    acc.append(metrics.accuracy_score(y_test, y_pred[i]))\n",
    "    print(f\"Acuuracy_{min_samples_leaf_list[i]}: {acc[i]}\")\n",
    "    print(f\"Feature importance of {min_samples_leaf_list[i]} min_samples_leaf:{clf[i].feature_importances_}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 根據自行調整的結果，得到的效果反而比較差，可能是因為對於數據不知如何調整，試著調整反而得到反效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared error: 0.065\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "# 讀取鳶尾花資料集\n",
    "wine = datasets.load_wine()\n",
    "\n",
    "# 切分訓練集/測試集\n",
    "x_train, x_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.25, random_state=4)\n",
    "\n",
    "# 建立模型\n",
    "clf = linear_model.LinearRegression()\n",
    "\n",
    "# 訓練模型\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "# 預測測試集\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "acc = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared error: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared error: 0.032\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestRegressor()\n",
    "\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(x_test)\n",
    "\n",
    "acc = metrics.mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared error: %.3f\" % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4615384615384615\n"
     ]
    }
   ],
   "source": [
    "print(0.030/0.065)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 使用 Random Forest Regressor 進行預測，得到的結果比使用 Linear Regression 效果好約 46.1%。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
